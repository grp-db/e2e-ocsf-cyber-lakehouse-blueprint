# E2E Cybersecurity Lakehouse - Pipeline Overview

## ğŸ¯ Purpose

**Medallion Architecture** (Bronze â†’ Silver â†’ Gold) that transforms raw audit logs into **OCSF-normalized** security events for SIEM integration and analytics.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS S3 / Azure ADLS /  â”‚    â”‚   KAFKA TOPICS                      â”‚
â”‚   Google Cloud Storage   â”‚    â”‚   (Real-time Streams)               â”‚
â”‚   (Auto Loader)          â”‚    â”‚   â€¢ <source>-audit-logs             â”‚
â”‚   /Volumes/<catalog>/    â”‚    â”‚                                     â”‚
â”‚   logs/<source>/         â”‚    â”‚                                     â”‚
â”‚   <source_type>/         â”‚    â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                 â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   ğŸ¥‰ BRONZE           |
              â”‚   Raw Ingestion       â”‚
              â”‚   â€¢ Variant column    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   ğŸ¥ˆ SILVER           â”‚
              â”‚   Parsed & Validated  â”‚
              â”‚   â€¢ Extract fields    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   ğŸ¥‡ GOLD             â”‚
              â”‚   OCSF Normalized     â”‚
              â”‚   â€¢ 6 unified tables  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Repository Structure

```
e2e-cyber-lakehouse/
â”œâ”€â”€ transformations/
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â””â”€â”€ <source>/<source_type>/
â”‚   â”‚       â”œâ”€â”€ bronze_<source>_<source_type>.py
â”‚   â”‚       â””â”€â”€ silver_<source>_<source_type>.py
â”‚   â”‚
â”‚   â””â”€â”€ mappings/
â”‚       â””â”€â”€ ocsf/iam/
â”‚           â”œâ”€â”€ gold_<source>_<source_type>.py
â”‚           â””â”€â”€ gold_ocsf_iam_event_classes.py  # Unified tables
â”‚
â”œâ”€â”€ utilities/
â”‚   â””â”€â”€ utils.py
â”‚
â””â”€â”€ _resources/
    â”œâ”€â”€ PIPELINE_OVERVIEW.md
    â””â”€â”€ OCSF_ARCHITECTURE.md
```

---

## ğŸ“Š Unity Catalog Structure

```
<catalog>
â”œâ”€â”€ <source>                        # Per-source databases
â”‚   â”œâ”€â”€ <source>_<source_type>_brz
â”‚   â””â”€â”€ <source>_<source_type>_slv
â”‚
â””â”€â”€ ocsf                            # OCSF Gold database
    â”œâ”€â”€ ocsf_iam_account_change         # 3001
    â”œâ”€â”€ ocsf_iam_authentication         # 3002
    â”œâ”€â”€ ocsf_iam_authorize_session      # 3003
    â”œâ”€â”€ ocsf_iam_entity_management      # 3004
    â”œâ”€â”€ ocsf_iam_user_access_management # 3005
    â””â”€â”€ ocsf_iam_group_management       # 3006
```

---

## ğŸ”„ Pipeline Layers

### ğŸ¥‰ Bronze - Raw Ingestion
- Ingest raw JSON with variant column (`data`)
- Minimal transformation, preserve original
- Extract metadata (_event_time, _event_date, _source)

### ğŸ¥ˆ Silver - Parsed & Validated
- Extract and flatten key fields from variant
- Data quality validation
- Source-specific schemas

### ğŸ¥‡ Gold - OCSF Normalized
- Transform to OCSF v1.7.0 IAM schema
- 6 unified tables (one per OCSF class)
- Multi-source append flows (GitHub + Slack + Atlassian â†’ single table)
- SIEM-ready output

---

## ğŸ’¾ Ingestion Patterns

### Auto Loader (Batch/Micro-batch)

```python
from pyspark import pipelines as sdp

@sdp.table(
    name="<source>_<source_type>_brz",
    cluster_by=["_event_date"],
    table_properties={
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.autoOptimize.autoCompact": "true"
    }
)
def bronze():
    return (
        spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("singleVariantColumn", "data")
        .load("/Volumes/<catalog>/logs/<source>/<source_type>/")
        .selectExpr(
            "CAST(try_variant_get(data, '$.timestamp', 'BIGINT') AS TIMESTAMP) as _event_time",
            "CAST(_event_time AS DATE) as _event_date",
            "'<source>' as _source",
            "'<source_type>' as _source_type",
            "data"
        )
    )
```

**Note**: SDP handles checkpointing and schema evolution automatically.

### Kafka (Real-time Streaming)

#### Configuration (Confluent Cloud Example)

```python
from pyspark import pipelines as sdp

# Confluent Kafka settings
confluent_bootstrap_servers = '<cluster-id>.us-east-1.aws.confluent.cloud:9092'
confluent_kafka_api_key = dbutils.secrets.get(scope='<scope>', key='<api-key>')
confluent_kafka_secret_key = dbutils.secrets.get(scope='<scope>', key='<secret-key>')

kafka_conf = {
    'kafka.bootstrap.servers': confluent_bootstrap_servers,
    'kafka.security.protocol': 'SASL_SSL',
    'kafka.sasl.jaas.config': f"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{confluent_kafka_api_key}' password='{confluent_kafka_secret_key}';",
    'kafka.ssl.endpoint.identification.algorithm': 'https',
    'kafka.sasl.mechanism': 'PLAIN',
    'startingOffsets': 'earliest',
    'failOnDataLoss': 'false',
}

topic_name = "<topic-name>"
```

#### Bronze Options

**Option 1: Binary (Raw)**
```python
@sdp.table(name="binary_logs_brz", cluster_by=["_event_date"])
def kafka_bronze_binary():
    return spark.readStream.format("kafka").option("subscribe", topic_name).options(**kafka_conf).load()
```

**Option 2: String (Decoded)**
```python
@sdp.table(name="string_logs_brz", cluster_by=["_event_date"])
def kafka_bronze_string():
    return (
        spark.readStream.format("kafka").option("subscribe", topic_name).options(**kafka_conf).load()
        .selectExpr("cast(key as string) as key", "cast(value as string) as value", "topic", "partition", "offset", "timestamp")
    )
```

**Option 3: Variant (Recommended)**
```python
@sdp.table(name="variant_logs_brz", cluster_by=["_event_date"])
def kafka_bronze_variant():
    return (
        spark.readStream.format("kafka").option("subscribe", topic_name).options(**kafka_conf).load()
        .selectExpr(
            "cast(key as string) as key",
            "from_json(cast(value as string), 'variant') as data",  # Parse to variant
            "topic", "partition", "offset", "timestamp"
        )
    )
```

**Recommendation**: Use **Variant** for audit logs - provides schema-on-read flexibility with `try_variant_get()`.

---

## ğŸ›¡ï¸ OCSF IAM Event Classes

| OCSF Class | UID | Purpose | Sources |
|------------|-----|---------|---------|
| Account Change | 3001 | User lifecycle | GitHub, Slack, Atlassian |
| Authentication | 3002 | Login/logout | GitHub, Slack, Atlassian |
| Authorize Session | 3003 | Access authorization | GitHub, Slack, Atlassian |
| Entity Management | 3004 | Resource lifecycle | Atlassian only |
| User Access Management | 3005 | Permission management | GitHub, Slack |
| Group Management | 3006 | Group/team operations | GitHub, Slack, Atlassian |

**Total**: 15 append flows (5 GitHub + 5 Slack + 5 Atlassian) â†’ 6 unified tables

**OCSF Category**: Identity & Access Management (UID: 3)  
**OCSF Version**: 1.7.0  
**Docs**: https://schema.ocsf.io/1.7.0/categories/iam

---

## ğŸ”‘ Key Technologies

- **[Spark Declarative Pipelines (SDP)](https://spark.apache.org/docs/4.1.0-preview1/declarative-pipelines-programming-guide.html)** - Declarative streaming ETL
- **Auto Loader / Kafka** - Batch or real-time ingestion
- **Delta Lake** - ACID transactions, auto-optimization
- **Unity Catalog** - Data governance
- **Variant Data Type** - Schema-on-read for JSON
- **OCSF v1.7.0** - Open security schema standard
